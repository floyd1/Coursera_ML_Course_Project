{"name":"Human Activity Recognition ML Project","tagline":"","body":"# Human Activity Recognition ML Project\r\nVladimir Goldin  \r\nAugust 22, 2015  \r\n\r\n### Background\r\n\r\nSix young healthy participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).  \r\nRead more: <http://groupware.les.inf.puc-rio.br/har#ixzz3jcYf6pTX>\r\n\r\nIn this project, we will attempt to use the HAR dataset to correctly classify the fashion in which the Unilateral Dumbbell Biceps Curl exercise was performed.\r\n\r\n### Load the data\r\n\r\n\r\n```r\r\nlibrary(caret)\r\n```\r\n\r\n```\r\n## Loading required package: lattice\r\n## Loading required package: ggplot2\r\n```\r\n\r\n```r\r\npml_full <- read.csv(\"pml-training.csv\")\r\n```\r\n\r\n### Feature Extraction\r\n\r\n`str(pml_full)` reveals that many features have `NA` or `#DIV/0!` values. Furthermore, some features are irrelevant for the prediction model (such as features `X`, `user_name`, etc.). And upon closer inspection, many others are derived from the collected instrument data, such as `avg`, `stddev`, etc. \r\n\r\nSo, to complete the feature extraction, we will keep only the primary instrument data, with the following derivative data removed: `avg`, `min`, `max`, `var`, `stddev`, `amplitude`, `kurtosis`, `skewness`\r\n\r\n\r\n```r\r\npml <- pml_full[,-c(1:7, grep(\"amplitude|min|max|avg|var|stddev|skewness|kurtosis\", names(pml_full), ignore.case = TRUE))]\r\n\r\ncols <- ncol(pml)\r\n```\r\n\r\nAs a result, of the 160 columns in the original dataset, we are left with 53 columns that we'll be using to build our prediction model.\r\n\r\n### Partitioning the Data\r\n\r\nWe will partition the original dataset into training and testing sets, 75% and 25%, respectively.\r\n\r\n\r\n```r\r\nset.seed(10505)\r\ninTrain <- createDataPartition(y = pml$classe, p = .75, list = FALSE)\r\ntraining <- pml[inTrain,]\r\ntesting <- pml[-inTrain,]\r\n```\r\n\r\n### Building a k-Nearest Neighbor Model with Cross-Validation\r\n\r\nWe've attempted to fit the data using the Partial Least Squares model, which yielded an accuracy rate of only 38%. With such a dismal result, we've decided to abandon the linear regression route altogether.\r\n\r\nInstead, since we are dealing with a classification problem, the kNN Model seemed like a sensible choice.\r\n\r\n\r\n```r\r\nknnFit <- train(training[,1:(cols-1)], training[,cols],\r\n                 method = \"knn\",\r\n                 preProcess = c(\"center\", \"scale\"),\r\n                 tuneLength = 10,\r\n                 trControl = trainControl(method = \"cv\"))\r\n\r\nknnFit\r\n```\r\n\r\n```\r\n## k-Nearest Neighbors \r\n## \r\n## 14718 samples\r\n##    52 predictor\r\n##     5 classes: 'A', 'B', 'C', 'D', 'E' \r\n## \r\n## Pre-processing: centered, scaled \r\n## Resampling: Cross-Validated (10 fold) \r\n## Summary of sample sizes: 13246, 13246, 13247, 13245, 13245, 13248, ... \r\n## Resampling results across tuning parameters:\r\n## \r\n##   k   Accuracy   Kappa      Accuracy SD  Kappa SD   \r\n##    5  0.9629719  0.9531520  0.003884553  0.004923691\r\n##    7  0.9506748  0.9375803  0.005222228  0.006619933\r\n##    9  0.9402109  0.9243324  0.007110269  0.009007607\r\n##   11  0.9311740  0.9128889  0.007601139  0.009646022\r\n##   13  0.9207772  0.8997230  0.007201140  0.009152958\r\n##   15  0.9126921  0.8894854  0.008346299  0.010601282\r\n##   17  0.9025027  0.8765767  0.008645326  0.010977378\r\n##   19  0.8948922  0.8669469  0.008548272  0.010849077\r\n##   21  0.8878939  0.8580893  0.007397837  0.009397475\r\n##   23  0.8800806  0.8481948  0.007578669  0.009620575\r\n## \r\n## Accuracy was used to select the optimal model using  the largest value.\r\n## The final value used for the model was k = 5.\r\n```\r\n\r\nFor k = 5, the accuracy was 96.3%.\r\n\r\n### Cross-Validation Plot\r\n\r\n\r\n```r\r\nplot(knnFit)\r\n```\r\n\r\n![Cross-Validation Plot](https://cloud.githubusercontent.com/assets/12988944/9427883/283cd60c-499c-11e5-83fd-7d0997aa3875.png)\r\n\r\nThe plot clearly shows that for k > 5, the model results in progressively worsening accuracy. This makes sense since our dataset consists of only 5 classes: {A, B, C, D, E}\r\n\r\n### Predicting on the `testing` set\r\n\r\n\r\n```r\r\ntest_knnFit <- predict(knnFit, newdata = testing[,1:(cols-1)])\r\n(conMat <- confusionMatrix(test_knnFit, testing[,cols]))\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1378   33    2    2    1\r\n##          B    7  885    4    0    7\r\n##          C    2   27  833   26    6\r\n##          D    4    0   14  774    6\r\n##          E    4    4    2    2  881\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9688          \r\n##                  95% CI : (0.9635, 0.9735)\r\n##     No Information Rate : 0.2845          \r\n##     P-Value [Acc > NIR] : < 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.9605          \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9878   0.9326   0.9743   0.9627   0.9778\r\n## Specificity            0.9892   0.9954   0.9849   0.9941   0.9970\r\n## Pos Pred Value         0.9732   0.9801   0.9318   0.9699   0.9866\r\n## Neg Pred Value         0.9951   0.9840   0.9945   0.9927   0.9950\r\n## Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837\r\n## Detection Rate         0.2810   0.1805   0.1699   0.1578   0.1796\r\n## Detection Prevalence   0.2887   0.1841   0.1823   0.1627   0.1821\r\n## Balanced Accuracy      0.9885   0.9640   0.9796   0.9784   0.9874\r\n```\r\n\r\nThe Confusion Matrix shows an accuracy of 96.9% on the `testing` dataset. And the error rate is 3.1%.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}