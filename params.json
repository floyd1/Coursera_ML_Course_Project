{"name":"Human Activity Recognition ML Project","tagline":"","body":"# Human Activity Recognition ML Project\r\nVladimir Goldin  \r\nAugust 22, 2015  \r\n\r\n### Background\r\n\r\nSix young healthy participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).  \r\nRead more: <http://groupware.les.inf.puc-rio.br/har#ixzz3jcYf6pTX>\r\n\r\nIn this project, we will attempt to use the HAR dataset to correctly classify the fashion in which the Unilateral Dumbbell Biceps Curl exercise was performed.\r\n\r\n### Load the data\r\n\r\n\r\n```r\r\nlibrary(caret)\r\n```\r\n\r\n```\r\n## Loading required package: lattice\r\n## Loading required package: ggplot2\r\n```\r\n\r\n```r\r\npml_full <- read.csv(\"pml-training.csv\")\r\n```\r\n\r\n### Feature Extraction\r\n\r\n`str(pml_full)` reveals that many features have `NA` or `#DIV/0!` values. Furthermore, some features are irrelevant for the prediction model (such as features `X`, `user_name`, etc.). And upon closer inspection, many others are derived from the collected instrument data, such as `avg`, `stddev`, etc. \r\n\r\nSo, to complete the feature extraction, we will keep only the primary instrument data, with the following derivative data removed: `avg`, `min`, `max`, `var`, `stddev`, `amplitude`, `kurtosis`, `skewness`\r\n\r\n\r\n```r\r\npml <- pml_full[,-c(1:7, grep(\"amplitude|min|max|avg|var|stddev|skewness|kurtosis\", names(pml_full), ignore.case = TRUE))]\r\n\r\ncols <- ncol(pml)\r\n```\r\n\r\nAs a result, of the 160 columns in the original dataset, we are left with 53 columns that we'll be using to build our prediction model.\r\n\r\n### Partitioning the Data\r\n\r\nWe will partition the original dataset into training and testing sets, 75% and 25%, respectively.\r\n\r\n\r\n```r\r\nset.seed(40505)\r\ninTrain <- createDataPartition(y = pml$classe, p = .75, list = FALSE)\r\ntraining <- pml[inTrain,]\r\ntesting <- pml[-inTrain,]\r\n```\r\n\r\n### Building a k-Nearest Neighbor Model with Cross-Validation\r\n\r\nWe've attempted to fit the data using the Partial Least Squares model, which yielded an accuracy rate of only 38%. With such a dismal result, we've decided to abandon the linear regression route altogether.\r\n\r\nInstead, since we are dealing with a classification problem, the kNN Model seemed like a sensible choice.\r\n\r\n\r\n```r\r\nknnFit <- train(training[,1:(cols-1)], training[,cols],\r\n                 method = \"knn\",\r\n                 preProcess = c(\"center\", \"scale\"),\r\n                 tuneLength = 10,\r\n                 trControl = trainControl(method = \"cv\"))\r\n\r\nsummary(knnFit)\r\n```\r\n\r\n```\r\n##             Length Class      Mode     \r\n## learn        2     -none-     list     \r\n## k            1     -none-     numeric  \r\n## theDots      0     -none-     list     \r\n## xNames      52     -none-     character\r\n## problemType  1     -none-     character\r\n## tuneValue    1     data.frame list     \r\n## obsLevels    5     -none-     character\r\n```\r\n\r\nFor k = 5, the accuracy was 96.7%.\r\n\r\n### Cross-Validation Plot\r\n\r\n\r\n```r\r\nplot(knnFit)\r\n```\r\n\r\n![Cross-Validation Plot](https://github.com/floyd1/Coursera_ML_Course_Project/tree/master/pml-project_files/figure-html/unnamed-chunk-5-1.png)\r\n\r\nThe plot clearly shows that for k > 5, the model results in progressively worsening accuracy. This makes sense since our dataset consists of only 5 classes: {A, B, C, D, E}\r\n\r\n### Predicting on the `testing` set\r\n\r\n\r\n```r\r\ntest_knnFit <- predict(knnFit, newdata = testing[,1:(cols-1)])\r\n(conMat <- confusionMatrix(test_knnFit, testing[,cols]))\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1379   13    1    2    0\r\n##          B    6  917   12    0    9\r\n##          C    2   13  828   33    4\r\n##          D    4    3   11  763    3\r\n##          E    4    3    3    6  885\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9731          \r\n##                  95% CI : (0.9682, 0.9774)\r\n##     No Information Rate : 0.2845          \r\n##     P-Value [Acc > NIR] : < 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.966           \r\n##  Mcnemar's Test P-Value : 0.004074        \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9885   0.9663   0.9684   0.9490   0.9822\r\n## Specificity            0.9954   0.9932   0.9872   0.9949   0.9960\r\n## Pos Pred Value         0.9885   0.9714   0.9409   0.9732   0.9822\r\n## Neg Pred Value         0.9954   0.9919   0.9933   0.9900   0.9960\r\n## Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837\r\n## Detection Rate         0.2812   0.1870   0.1688   0.1556   0.1805\r\n## Detection Prevalence   0.2845   0.1925   0.1794   0.1599   0.1837\r\n## Balanced Accuracy      0.9920   0.9797   0.9778   0.9719   0.9891\r\n```\r\n\r\nThe Confusion Matrix shows an accuracy of 97.3% on the `testing` dataset. And the error rate is 2.7%.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}