<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Human Activity Recognition ML Project by floyd1</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Human Activity Recognition ML Project</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/floyd1/Coursera_ML_Course_Project" class="btn">View on GitHub</a>
      <a href="https://github.com/floyd1/Coursera_ML_Course_Project/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/floyd1/Coursera_ML_Course_Project/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="human-activity-recognition-ml-project" class="anchor" href="#human-activity-recognition-ml-project" aria-hidden="true"><span class="octicon octicon-link"></span></a>Human Activity Recognition ML Project</h1>

<p>Vladimir Goldin<br>
August 22, 2015  </p>

<h3>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span class="octicon octicon-link"></span></a>Background</h3>

<p>Six young healthy participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).<br>
Read more: <a href="http://groupware.les.inf.puc-rio.br/har#ixzz3jcYf6pTX">http://groupware.les.inf.puc-rio.br/har#ixzz3jcYf6pTX</a></p>

<p>In this project, we will attempt to use the HAR dataset to correctly classify the fashion in which the Unilateral Dumbbell Biceps Curl exercise was performed.</p>

<h3>
<a id="load-the-data" class="anchor" href="#load-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Load the data</h3>

<div class="highlight highlight-r"><pre>library(<span class="pl-smi">caret</span>)</pre></div>

<pre><code>## Loading required package: lattice
## Loading required package: ggplot2
</code></pre>

<div class="highlight highlight-r"><pre><span class="pl-smi">pml_full</span> <span class="pl-k">&lt;-</span> read.csv(<span class="pl-s"><span class="pl-pds">"</span>pml-training.csv<span class="pl-pds">"</span></span>)</pre></div>

<h3>
<a id="feature-extraction" class="anchor" href="#feature-extraction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Feature Extraction</h3>

<p><code>str(pml_full)</code> reveals that many features have <code>NA</code> or <code>#DIV/0!</code> values. Furthermore, some features are irrelevant for the prediction model (such as features <code>X</code>, <code>user_name</code>, etc.). And upon closer inspection, many others are derived from the collected instrument data, such as <code>avg</code>, <code>stddev</code>, etc. </p>

<p>So, to complete the feature extraction, we will keep only the primary instrument data, with the following derivative data removed: <code>avg</code>, <code>min</code>, <code>max</code>, <code>var</code>, <code>stddev</code>, <code>amplitude</code>, <code>kurtosis</code>, <code>skewness</code></p>

<div class="highlight highlight-r"><pre><span class="pl-smi">pml</span> <span class="pl-k">&lt;-</span> <span class="pl-smi">pml_full</span>[,<span class="pl-k">-</span>c(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">7</span>, grep(<span class="pl-s"><span class="pl-pds">"</span>amplitude|min|max|avg|var|stddev|skewness|kurtosis<span class="pl-pds">"</span></span>, names(<span class="pl-smi">pml_full</span>), <span class="pl-v">ignore.case</span> <span class="pl-k">=</span> <span class="pl-c1">TRUE</span>))]

<span class="pl-smi">cols</span> <span class="pl-k">&lt;-</span> ncol(<span class="pl-smi">pml</span>)</pre></div>

<p>As a result, of the 160 columns in the original dataset, we are left with 53 columns that we'll be using to build our prediction model.</p>

<h3>
<a id="partitioning-the-data" class="anchor" href="#partitioning-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Partitioning the Data</h3>

<p>We will partition the original dataset into training and testing sets, 75% and 25%, respectively.</p>

<div class="highlight highlight-r"><pre>set.seed(<span class="pl-c1">10505</span>)
<span class="pl-smi">inTrain</span> <span class="pl-k">&lt;-</span> createDataPartition(<span class="pl-v">y</span> <span class="pl-k">=</span> <span class="pl-smi">pml</span><span class="pl-k">$</span><span class="pl-smi">classe</span>, <span class="pl-v">p</span> <span class="pl-k">=</span> .<span class="pl-c1">75</span>, <span class="pl-v">list</span> <span class="pl-k">=</span> <span class="pl-c1">FALSE</span>)
<span class="pl-smi">training</span> <span class="pl-k">&lt;-</span> <span class="pl-smi">pml</span>[<span class="pl-smi">inTrain</span>,]
<span class="pl-smi">testing</span> <span class="pl-k">&lt;-</span> <span class="pl-smi">pml</span>[<span class="pl-k">-</span><span class="pl-smi">inTrain</span>,]</pre></div>

<h3>
<a id="building-a-k-nearest-neighbor-model-with-cross-validation" class="anchor" href="#building-a-k-nearest-neighbor-model-with-cross-validation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Building a k-Nearest Neighbor Model with Cross-Validation</h3>

<p>We've attempted to fit the data using the Partial Least Squares model, which yielded an accuracy rate of only 38%. With such a dismal result, we've decided to abandon the linear regression route altogether.</p>

<p>Instead, since we are dealing with a classification problem, the kNN Model seemed like a sensible choice.</p>

<div class="highlight highlight-r"><pre><span class="pl-smi">knnFit</span> <span class="pl-k">&lt;-</span> train(<span class="pl-smi">training</span>[,<span class="pl-c1">1</span><span class="pl-k">:</span>(<span class="pl-smi">cols</span><span class="pl-k">-</span><span class="pl-c1">1</span>)], <span class="pl-smi">training</span>[,<span class="pl-smi">cols</span>],
                 <span class="pl-v">method</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>knn<span class="pl-pds">"</span></span>,
                 <span class="pl-v">preProcess</span> <span class="pl-k">=</span> c(<span class="pl-s"><span class="pl-pds">"</span>center<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>scale<span class="pl-pds">"</span></span>),
                 <span class="pl-v">tuneLength</span> <span class="pl-k">=</span> <span class="pl-c1">10</span>,
                 <span class="pl-v">trControl</span> <span class="pl-k">=</span> trainControl(<span class="pl-v">method</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>cv<span class="pl-pds">"</span></span>))

<span class="pl-smi">knnFit</span></pre></div>

<pre><code>## k-Nearest Neighbors 
## 
## 14718 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## Pre-processing: centered, scaled 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 13246, 13246, 13247, 13245, 13245, 13248, ... 
## Resampling results across tuning parameters:
## 
##   k   Accuracy   Kappa      Accuracy SD  Kappa SD   
##    5  0.9629719  0.9531520  0.003884553  0.004923691
##    7  0.9506748  0.9375803  0.005222228  0.006619933
##    9  0.9402109  0.9243324  0.007110269  0.009007607
##   11  0.9311740  0.9128889  0.007601139  0.009646022
##   13  0.9207772  0.8997230  0.007201140  0.009152958
##   15  0.9126921  0.8894854  0.008346299  0.010601282
##   17  0.9025027  0.8765767  0.008645326  0.010977378
##   19  0.8948922  0.8669469  0.008548272  0.010849077
##   21  0.8878939  0.8580893  0.007397837  0.009397475
##   23  0.8800806  0.8481948  0.007578669  0.009620575
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was k = 5.
</code></pre>

<p>For k = 5, the accuracy was 96.3%.</p>

<h3>
<a id="cross-validation-plot" class="anchor" href="#cross-validation-plot" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cross-Validation Plot</h3>

<div class="highlight highlight-r"><pre>plot(<span class="pl-smi">knnFit</span>)</pre></div>

<p><img src="https://cloud.githubusercontent.com/assets/12988944/9427883/283cd60c-499c-11e5-83fd-7d0997aa3875.png" alt="Cross-Validation Plot"></p>

<p>The plot clearly shows that for k &gt; 5, the model results in progressively worsening accuracy. This makes sense since our dataset consists of only 5 classes: {A, B, C, D, E}</p>

<h3>
<a id="predicting-on-the-testing-set" class="anchor" href="#predicting-on-the-testing-set" aria-hidden="true"><span class="octicon octicon-link"></span></a>Predicting on the <code>testing</code> set</h3>

<div class="highlight highlight-r"><pre><span class="pl-smi">test_knnFit</span> <span class="pl-k">&lt;-</span> predict(<span class="pl-smi">knnFit</span>, <span class="pl-v">newdata</span> <span class="pl-k">=</span> <span class="pl-smi">testing</span>[,<span class="pl-c1">1</span><span class="pl-k">:</span>(<span class="pl-smi">cols</span><span class="pl-k">-</span><span class="pl-c1">1</span>)])
(<span class="pl-smi">conMat</span> <span class="pl-k">&lt;-</span> confusionMatrix(<span class="pl-smi">test_knnFit</span>, <span class="pl-smi">testing</span>[,<span class="pl-smi">cols</span>]))</pre></div>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1378   33    2    2    1
##          B    7  885    4    0    7
##          C    2   27  833   26    6
##          D    4    0   14  774    6
##          E    4    4    2    2  881
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9688          
##                  95% CI : (0.9635, 0.9735)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9605          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9878   0.9326   0.9743   0.9627   0.9778
## Specificity            0.9892   0.9954   0.9849   0.9941   0.9970
## Pos Pred Value         0.9732   0.9801   0.9318   0.9699   0.9866
## Neg Pred Value         0.9951   0.9840   0.9945   0.9927   0.9950
## Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
## Detection Rate         0.2810   0.1805   0.1699   0.1578   0.1796
## Detection Prevalence   0.2887   0.1841   0.1823   0.1627   0.1821
## Balanced Accuracy      0.9885   0.9640   0.9796   0.9784   0.9874
</code></pre>

<p>The Confusion Matrix shows an accuracy of 96.9% on the <code>testing</code> dataset. And the error rate is 3.1%.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/floyd1/Coursera_ML_Course_Project">Human Activity Recognition ML Project</a> is maintained by <a href="https://github.com/floyd1">floyd1</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
